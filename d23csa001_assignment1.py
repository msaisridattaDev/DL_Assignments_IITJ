# -*- coding: utf-8 -*-
"""D23CSA001.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eclkZkh2VuTSb01WeZIAYcA9gAeUjzKt
"""

import torch
import matplotlib.pyplot as plt
import torchvision
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import seaborn as sns

dir = '/home/maverick/MTech_sem2/DL_assign_1/train-images.idx3-ubyte'

labels_dir = '/home/maverick/MTech_sem2/DL_assign_1/train-labels.idx1-ubyte'

test_image_dir = '/home/maverick/MTech_sem2/DL_assign_1/t10k-images.idx3-ubyte'
test_labels_dir = '/home/maverick/MTech_sem2/DL_assign_1/t10k-labels.idx1-ubyte'

!pip install idx2numpy

import idx2numpy

Data = idx2numpy.convert_from_file(dir)

labels = idx2numpy.convert_from_file(labels_dir)

test_data = idx2numpy.convert_from_file(test_image_dir)
test_labels = idx2numpy.convert_from_file(test_labels_dir)

test_labels

Data =  Data/255
test_data = test_data/255
test_data = torch.from_numpy(test_data).float()

test_labels = torch.from_numpy(test_labels).type(torch.LongTensor)

first_image = Data[0]

plt.imshow(first_image,cmap='Blues_r')

test_first_image = test_data[0]
test_first_image_label = test_labels[0]
test_first_image_label

plt.imshow(test_first_image,cmap='Blues_r')
plt.title(test_first_image_label)

X_train, X_val, y_train, y_val = train_test_split(Data,labels,test_size=0.2,random_state=42)

X_train = torch.from_numpy(X_train)
y_train = torch.from_numpy(y_train).type(torch.LongTensor)

X_val = torch.from_numpy(X_val).float()
y_val = torch.from_numpy(y_val).type(torch.LongTensor)

y_val.dtype

X_train = X_train.type(torch.float32)

X_train.dtype

from torch.utils.data import DataLoader

train_tensor = torch.utils.data.TensorDataset(X_train, y_train)
val_tensor = torch.utils.data.TensorDataset(X_val, y_val)
test_tensor =  torch.utils.data.TensorDataset(test_data,test_labels)

train_data = DataLoader(train_tensor,batch_size=20,shuffle=False,num_workers=1)
val_data   = DataLoader(val_tensor,batch_size=20,shuffle=False,num_workers=1)
X_test = DataLoader(test_tensor,batch_size=20,shuffle=False,num_workers=1)

import torch.nn as nn

class CNN(nn.Module):
    def __init__(self):
        super(CNN,self).__init__()


        self.convolution1 = nn.Sequential(
            nn.Conv2d(in_channels=1,out_channels=16,kernel_size=7,stride=1,padding=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2,stride=2)
            )

        self.convolution2 = nn.Sequential(
            nn.Conv2d(in_channels=16,out_channels=8,kernel_size=5,stride=1,padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2,stride=2)
        )

        self.convolution3 = nn.Sequential(
            nn.Conv2d(in_channels=8,out_channels=4,kernel_size=3,stride=2,padding=1),
            nn.ReLU(),
            nn.AvgPool2d(kernel_size=2,stride=2)
        )
        #nn.Softmax()
        self.fully_connected_layer1 = nn.Linear(4*2*2,64)
        self.fully_connected_layer2 = nn.Linear(64,32)
        self.fully_connected_layer3 = nn.Linear(32,10)

    def forward(self,x):
        relu = nn.ReLU()
        x = self.convolution1(x)
        x = self.convolution2(x)
        x = self.convolution3(x)

        x = x.view(x.size(0), -1)
        # output = nn.Softmax(x)
        x = self.fully_connected_layer1(x)
        x = relu(x)
        x = self.fully_connected_layer2(x)
        x = relu(x)
        output = self.fully_connected_layer3(x)


        return output

torch.manual_seed(24)

cnn = CNN()
cnn

from torch.autograd import Variable

optimizer = torch.optim.Adam(cnn.parameters(), lr=0.003)
find_loss = nn.CrossEntropyLoss()
loss = []
train_loss = 0.0
validation_loss = []
validation_accuracy = []
test_loss_lst= []
test_accuracy_lst = []

def validate(validation_data):
    cnn.eval()
    validation_prediction = torch.LongTensor()

    validation_current_loss = 0.0
    validation_correct_predictions = 0

    for images_val,label_val in validation_data:
        images_val,label_val = Variable(images_val.view(20,1,28,28),volatile=True) , Variable(label_val)

        output = cnn(images_val)
        loss = find_loss(output,label_val)

        validation_current_loss += loss.item()
        predictions = output.data.max(1,keepdim=True)[1]

        validation_correct_predictions += predictions.eq(label_val.data.view_as(predictions)).cpu().sum()

        validation_prediction = torch.cat((validation_prediction,predictions),dim=0)

    val_loss = validation_current_loss/len(validation_data.dataset)
    val_accuracy = 100. * validation_correct_predictions/len(validation_data.dataset)
    return val_loss,val_accuracy.item()

def prediction_TestData(test_x):
    cnn.eval()

    test_predictions = torch.LongTensor()
    labels_test = torch.LongTensor()

    test_current_loss = 0.0
    test_correct_predictions = 0

    for i, (images,labels) in enumerate(test_x):
        images,labels = Variable(images.view(20,1,28,28),volatile=True),Variable(labels)

        output = cnn(images)
        loss = find_loss(output,labels)

        test_current_loss += loss.item()
        predictions = output.data.cpu().max(1,keepdim = True)[1]
        test_correct_predictions += (predictions == labels.view_as(predictions)).sum().item()
        test_predictions = torch.cat((test_predictions,predictions),dim=0)
        labels_test      = torch.cat((labels_test,labels),dim=0)
    test_loss = test_current_loss/len(test_x.dataset)
    accuracy = 100 * test_correct_predictions / len(test_x.dataset)
    return test_loss,accuracy,test_predictions,labels_test

def train(num_epochs):
    cnn.train()
    for epoch in range(num_epochs):
        train_batch_loss = 0.0
        correct_predictions = 0.0
        for i, (X,y) in enumerate(train_data):
            x_batch,y_batch = Variable(X.view(20,1,28,28)), Variable(y)
            optimizer.zero_grad()
            output = cnn(x_batch)

            current_loss = find_loss(output,y_batch)


            # loss.append(current_loss.item())
            train_batch_loss += current_loss.item()
            _,predictions = torch.max(output.data,1)
            correct_predictions += (predictions == y_batch).sum().item()

            current_loss.backward()
            optimizer.step()

            if (i+1) %500==0:
                print("Train epoch {} loss :{}".format(epoch+1,current_loss))

        val_loss,val_accuracy = validate(val_data)
        validation_loss.append(val_loss)
        validation_accuracy.append(val_accuracy)
        loss.append(train_batch_loss/len(train_data.dataset))
        train_accuracy = 100 * correct_predictions/len(train_data.dataset)

        test_loss,test_accuracy,test_predictions,labels_test = prediction_TestData(X_test)
        test_loss_lst.append(test_loss)
        test_accuracy_lst.append(test_accuracy)
    return train_accuracy,test_predictions,labels_test

train_accuracy, test_predictions,labels_test =train(10)

plt.plot(loss)



plt.plot(validation_loss)

train_accuracy,validation_accuracy[-1]

plt.plot(validation_accuracy)

test_accuracy_lst[-1]

plt.plot(test_loss_lst)

conf_matrix = confusion_matrix(labels_test,test_predictions)

# Plot confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues_r', xticklabels=np.arange(10), yticklabels=np.arange(10))
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

torch.save(cnn.state_dict(), "best_model_task1.pt")

"""### Task 2 starts here"""

import torch
import matplotlib.pyplot as plt
import torchvision
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import seaborn as sns

dir = '/home/maverick/MTech_sem2/DL_assign_1/train-images.idx3-ubyte'

labels_dir = '/home/maverick/MTech_sem2/DL_assign_1/train-labels.idx1-ubyte'

test_image_dir = '/home/maverick/MTech_sem2/DL_assign_1/t10k-images.idx3-ubyte'
test_labels_dir = '/home/maverick/MTech_sem2/DL_assign_1/t10k-labels.idx1-ubyte'

!pip install idx2numpy

import idx2numpy

Data = idx2numpy.convert_from_file(dir)

labels = idx2numpy.convert_from_file(labels_dir)

test_data = idx2numpy.convert_from_file(test_image_dir)
test_labels = idx2numpy.convert_from_file(test_labels_dir)

test_labels

class_mapping = {
    0: 0,
    1: 1,
    2: 2,
    3: 2,
    4: 3,
    5: 2,
    6: 0,
    7: 1,
    8: 2,
    9: 3,
}

mapped_labels_train = np.array([class_mapping[label] for label in labels])
mapped_labels_test = np.array([class_mapping[label] for label in test_labels])

Data =  Data/255
test_data = test_data/255
test_data = torch.from_numpy(test_data).float()

first_image = Data[0]

plt.imshow(first_image,cmap='Blues_r')

test_first_image = test_data[0]
test_first_image_label = test_labels[0]
test_first_image_label

plt.imshow(test_first_image,cmap='Blues_r')
plt.title(test_first_image_label)

X_train, X_val, y_train, y_val = train_test_split(Data,mapped_labels_train,test_size=0.2,random_state=42)

X_train = torch.from_numpy(X_train)
y_train = torch.from_numpy(y_train).type(torch.LongTensor)

X_val = torch.from_numpy(X_val).float()
y_val = torch.from_numpy(y_val).type(torch.LongTensor)

mapped_labels_test = torch.from_numpy(mapped_labels_test).type(torch.LongTensor)

X_train = X_train.type(torch.float32)

X_train.dtype

from torch.utils.data import DataLoader

train_tensor = torch.utils.data.TensorDataset(X_train, y_train)
val_tensor = torch.utils.data.TensorDataset(X_val, y_val)
test_tensor =  torch.utils.data.TensorDataset(test_data,mapped_labels_test)

train_data = DataLoader(train_tensor,batch_size=20,shuffle=False,num_workers=1)
val_data   = DataLoader(val_tensor,batch_size=20,shuffle=False,num_workers=1)
X_test = DataLoader(test_tensor,batch_size=20,shuffle=False,num_workers=1)

import torch.nn as nn

class CNN(nn.Module):
    def __init__(self):
        super(CNN,self).__init__()


        self.convolution1 = nn.Sequential(
            nn.Conv2d(in_channels=1,out_channels=16,kernel_size=7,stride=1,padding=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2,stride=2)
            )

        self.convolution2 = nn.Sequential(
            nn.Conv2d(in_channels=16,out_channels=8,kernel_size=5,stride=1,padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2,stride=2)
        )

        self.convolution3 = nn.Sequential(
            nn.Conv2d(in_channels=8,out_channels=4,kernel_size=3,stride=2,padding=1),
            nn.ReLU(),
            nn.AvgPool2d(kernel_size=2,stride=2)
        )
        #nn.Softmax()
        self.fully_connected_layer1 = nn.Linear(4*2*2,64)
        self.fully_connected_layer2 = nn.Linear(64,32)
        self.fully_connected_layer3 = nn.Linear(32,10)
        self.fully_connected_layer4 = nn.Linear(10,4)

    def forward(self,x):
        relu = nn.ReLU()
        x = self.convolution1(x)
        x = self.convolution2(x)
        x = self.convolution3(x)

        x = x.view(x.size(0), -1)
        # output = nn.Softmax(x)
        x = self.fully_connected_layer1(x)
        x = relu(x)
        x = self.fully_connected_layer2(x)
        x = relu(x)
        x = self.fully_connected_layer3(x)
        x = relu(x)
        output = self.fully_connected_layer4(x)


        return output

torch.manual_seed(24)

cnn = CNN()
cnn

from torch.autograd import Variable

optimizer = torch.optim.Adam(cnn.parameters(), lr=0.003)
find_loss = nn.CrossEntropyLoss()
loss = []
train_loss = 0.0
validation_loss = []
validation_accuracy = []
test_loss_lst= []
test_accuracy_lst = []

def validate(validation_data):
    cnn.eval()
    validation_prediction = torch.LongTensor()

    validation_current_loss = 0.0
    validation_correct_predictions = 0

    for images_val,label_val in validation_data:
        images_val,label_val = Variable(images_val.view(20,1,28,28),volatile=True) , Variable(label_val)

        output = cnn(images_val)
        loss = find_loss(output,label_val)

        validation_current_loss += loss.item()
        predictions = output.data.max(1,keepdim=True)[1]

        validation_correct_predictions += predictions.eq(label_val.data.view_as(predictions)).cpu().sum()

        validation_prediction = torch.cat((validation_prediction,predictions),dim=0)

    val_loss = validation_current_loss/len(validation_data.dataset)
    val_accuracy = 100. * validation_correct_predictions/len(validation_data.dataset)
    return val_loss,val_accuracy.item()

def prediction_TestData(test_x):
    cnn.eval()

    test_predictions = torch.LongTensor()
    labels_test = torch.LongTensor()

    test_current_loss = 0.0
    test_correct_predictions = 0

    for i, (images,labels) in enumerate(test_x):
        images,labels = Variable(images.view(20,1,28,28),volatile=True),Variable(labels)

        output = cnn(images)
        loss = find_loss(output,labels)

        test_current_loss += loss.item()
        predictions = output.data.cpu().max(1,keepdim = True)[1]
        test_correct_predictions += (predictions == labels.view_as(predictions)).sum().item()
        test_predictions = torch.cat((test_predictions,predictions),dim=0)
        labels_test      = torch.cat((labels_test,labels),dim=0)
    test_loss = test_current_loss/len(test_x.dataset)
    accuracy = 100 * test_correct_predictions / len(test_x.dataset)
    return test_loss,accuracy,test_predictions,labels_test

def train(num_epochs):
    cnn.train()
    for epoch in range(num_epochs):
        train_batch_loss = 0.0
        correct_predictions = 0.0
        for i, (X,y) in enumerate(train_data):
            x_batch,y_batch = Variable(X.view(20,1,28,28)), Variable(y)
            optimizer.zero_grad()
            output = cnn(x_batch)

            current_loss = find_loss(output,y_batch)


            # loss.append(current_loss.item())
            train_batch_loss += current_loss.item()
            _,predictions = torch.max(output.data,1)
            correct_predictions += (predictions == y_batch).sum().item()

            current_loss.backward()
            optimizer.step()

            if (i+1) %500==0:
                print("Train epoch {} loss :{}".format(epoch+1,current_loss))

        val_loss,val_accuracy = validate(val_data)
        validation_loss.append(val_loss)
        validation_accuracy.append(val_accuracy)
        loss.append(train_batch_loss/len(train_data.dataset))
        train_accuracy = 100 * correct_predictions/len(train_data.dataset)

        test_loss,test_accuracy,test_predictions,labels_test = prediction_TestData(X_test)
        test_loss_lst.append(test_loss)
        test_accuracy_lst.append(test_accuracy)
    return train_accuracy,test_predictions,labels_test

train_accuracy, test_predictions,labels_test =train(10)

plt.plot(loss)



plt.plot(validation_loss)

train_accuracy,validation_accuracy[-1]

plt.plot(validation_accuracy)

test_accuracy_lst[-1]

plt.plot(test_loss_lst)

conf_matrix = confusion_matrix(labels_test,test_predictions)

# Plot confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues_r', xticklabels=np.arange(4), yticklabels=np.arange(4))
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

torch.save(cnn.state_dict(), "best_model_task2.pt")

